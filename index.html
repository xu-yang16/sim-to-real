<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Sim-to-Real Learning for Legged Locomotion</title>
<meta name="author" content="Xu Yang"/>
<meta name="description" content="group meeting on 2022.09.28"/>
<meta name="keywords" content="RL; sim-to-real; legged locomotion"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./reveal.js/dist/reveal.css"/>
<link rel="stylesheet" href="./reveal.js/dist/theme/oer-reveal.css" id="theme"/>
<link rel="stylesheet" href="./reveal.js/plugin/accessibility/helper.css"/>
<link rel="stylesheet" href="./reveal.js/plugin/toc-progress/toc-progress.css"/>
<link rel="stylesheet" href="./reveal.js/dist/theme/toc-style.css"/>
<link rel="stylesheet" href="./reveal.js/dist/theme/fonts/source-sans-pro/source-sans-pro.css"/>
<link rel="stylesheet" href="./reveal.js/plugin/mine/font-awesome.min.css">
</head>
<body prefix="dc: http://purl.org/dc/elements/1.1/ dcterms: http://purl.org/dc/terms/ dcmitype: http://purl.org/dc/dcmitype/ cc: http://creativecommons.org/ns#" typeof="dcmitype:InteractiveResource">
	<style>
		:root{
			--myred: #981E32;
		}
		:root{
			--myblue: #0582CA;
		}
	</style>
<div class="reveal">
<div class="slides">

<section>
<section id="sec-title-slide" data-state="no-toc-progress"  class="center">
<!-- The following copyright information only applies to the title slide. -->
<!-- SPDX-FileCopyrightText: 2017-2019 Jens Lechtenbörger -->
<!-- SPDX-License-Identifier: CC0-1.0 -->
<!-- This is an HTML template for the title slide. -->
<!-- Embed logos as necessary. -->
<!-- <a class="nooutlink" href="url"><img class="state-background your-logo-class" src="whatever.png" alt="Whatever" /></a> -->

<div class="talk-title">
    <h1 class="no-toc-progress" style="text-align:center;">Sim-to-Real Learning for Legged Locomotion</h1>
</div>
<div class="talk-subtitle">
    <p></p>
</div>
<div class="talk-author">
  	<p style="font-size:55px;text-align:center;">
	  	Xu Yang
	</p>
	<p style="font-size:35px;text-align:center;">
		<strong style="color:black;">Department of Automation, Tsinghua University<br></br></strong>
	</p>
</div>
</section>

<section id="slide-content" class="center">
	<h3 id="content">Table of Contents</h3>
	<div id="text-table-of-contents">
	<ul>
		<li><a href="#slide-intro">Introduction</a></li>
		<li><a href="#slide-anymal">Case study: Anymal</a></li>
		<li><a href="#slide-cassie">Case study: Cassie</a></li>
		<li><a href="#slide-conclusion">Conclusion</a></li>
	</ul>
	</div>
	
	<div class="slide-footer"><br></div>
</section>

<!--Background-->
<section id="slide-intro" class="center">
	<h3 id="h3_intro" hidden>Introduction</h3>
	<h4 id="h4_intro">Introduction</h4>
    <div>
        <ul style="font-size:40px;">
            <li><strong><em style="color:var(--myred);">Reward function</em></strong>.<br>
                How to design reward functions to obtain expected controller performance?
            </li>
            <li><strong><em style="color:var(--myred);">Input and output</em></strong>.<br>
                What can you measure and what can you control?
            </li>
            <li><strong><em style="color:var(--myred);">Modelling</em></strong>.<br>
                simuation fidelity (rigid-body dynamics and actuator dynamics); dynamics randomization
            </li>
            <li><strong><em style="color:var(--myred);">Training</em></strong>.<br>
                parallel differentiable simulation; curriculum; teacher and student
            </li>
            <li><strong><em style="color:var(--myred);">Learning algorithm</em></strong>.<br>
                PPO etc.
            </li>
            <li><strong><em style="color:var(--myred);">Network design</em></strong>.<br>

            </li>
        </ul>
    </div>
</section>
<section id="slide-intro-table" class="center" data-background-image="./figures/review.png">
    <!-- <h4 id="h4_intro">Introduction</h4> -->
    <!-- <div class="r-stack">
        <img src="./figures/review.png" width="1000" height="1000">
    </div> -->
</section>
<!--Anymal-->
<section id="slide-anymal-traditional" class="center">
	<h3 id="h3_anymal" hidden>Case study: Anymal</h3>
	<h4 id="h4_anymal">Qradruped Robot: Traditional Controller Design</h4>
    <div>
        <ul style="font-size:40px;">
            <li>Goal: valid gait patterns (contact sequences) and commanded velocity tracking.</li>
            <li>Controller: <strong><em style="color:var(--myred);">planning</em></strong> and <strong><em style="color:var(--myred);">tracking</em></strong>.<br>
                
            </li>
            <li>Trajectory optimization for a <strong><em style="color:var(--myred);">complex rigid-body model</em></strong> with many unspecified <strong><em style="color:var(--myred);">contact points</em></strong> is beyond the capabilities of current optimization techniques. <strong><em style="color:var(--myblue);">Approximations</em></strong> are needed to reduce complexity.
                <ul>
                    <li>Rigid-body model: point mass, whole body...</li>
                    <li>Contact points: smooth contact dynamics...</li>
                </ul>
            </li>
        </ul>
    </div>
</section>
<section id="slide-anymal-motor-idea" class="center">
	<h4 id="h4_anymal">Anymal: Learning Motor Skills<a class="org-ref-reference" href="#slide-bibliography">[Hwangbo2019]</a></h4>
    <div class="leftcol" style="margin-top: 0%;">
        <ul style="font-size:35px;">
            <li>Goal: commanded velocity tracking and recovery from a fall.</li>
            <li>Modelling: rigid-body dynamics algorithms + contact solver + collision-detection library + <strong><em style="color:var(--myred);">actuator dynamics</em></strong>
            <li>Challenge: motors (SEAs) are hard to model.
            </li>
        </ul>
    </div>
    <div class="leftcol" style="margin-top: 0%;">
        <img src="./figures/anymal_motor_idea.png" >
    </div>
</section>
<section id="slide-anymal-motor" class="center">
	<h4 id="h4_anymal">Anymal: Control and State Estimation of Motors</h4>
    <div class="leftcol" style="margin-top: 0%;">
        <ul style="font-size:34px;">
            <li>Actuator network: 
                <ul>
                    <li>input: <strong><em style="color:var(--myred);">joint state history</em></strong> (including joint velocity and joint position error), which consists of the current state and 2 past states that correspond to $t-0.01$ and $t-0.02$ s.</li>
                    <li>the length of the history should be sufficiently longer than the sum of all communication delays and the mechanical responce time.</li>
                    <li>output: <strong><em style="color:var(--myred);">joint torque</em></strong></li>
                    <li>architecture: an MLP with 3 hidden layers of 32 units each; softsign activation function (12.2 $\mu$s) instead of tanh (31.6$\mu$s).</li>
                </ul>
            </li>
            
            <li>Dataset: the excitation must cover a wide range of frequency spectra.
                <ul>
                    <li>Foot trajectories in the form of a sine wavewith varied amplitude (5 to 10 cm) and frequency (1 to 25 Hz).</li>
                    <li>Constant contact with the ground.</li>
                    <li>The robot is disturbed manually.</li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="leftcol" style="margin-top: 0%;">
        <img src="./figures/anymal_motor_diagram.png" width="450" height="700">
    </div>
</section>
<section id="slide-anymal-torque" class="center">
	<h4 id="h4_anymal">Anymal: Control and State Estimation of Motors</h4>
    <div class="r-stack">
        <img src="./figures/anymal_motor_torque.png">
    </div>
</section>
<section id="slide-anymal-policy" class="center">
	<h4 id="h4_anymal">Anymal: Policy </h4>
    <div class="leftcol" style="margin-top: 0%;">
        <ul style="font-size:35px;">
            <li>Hyrbid simulator: 
                <ul>
                    <li>nearly 50 000 time steps per second (half of the run time is used to evaluate the actuator nets)</li>
                </ul>
            </li>
            <li>Policy network:
                <ul>
                    <li>input: <strong><em style="color:var(--myred);">$(o_k=<\Phi^g, r_z, v, \omega, \Phi, \dot\Phi, \Theta, a_{k-1}, C>)$</em></strong></li>
					<ul>
						<li>Body twists $\Phi^g$ (direction of the gravity vector in the IMU frame); </li>
						<li>height, linear and angular veclocities of the base; joint angles, velocities; </li>
						<li>joint state history $\Theta$ sparsely sampled at $t_k-0.01$ s to $t_k-0.002$ s; (enabling contact detection, which could be replaced by force sensors)</li>
						<li>previous action; command $C$.</li>
					</ul>
                    <li>output: <strong><em style="color:var(--myred);">joint position target</em></strong> (why not torques or velocities?)</li>
                    <li>architecture: an MLP with two hidden layers, with 256 and 128 units each and tanh nonlinearity (bounded activation functions yield less aggressive trajectories for real robots when subjected disturbances)</li>
            </li>
        </ul>
    </div>
    <div class="rightcol" style="margin-top: 0%;">
        <img src="./figures/anymal_motor_diagram.png" width="450" height="700">
    </div>
</section>
<section id="slide-Peng2017" class="center">
	<h4 id="h4_anymal">Learning Locomotion Skills: Does the Choice of Action Space Matter?<a class="org-ref-reference" href="#slide-bibliography">[Peng2017]</a></h4>
    <div class="leftcol" style="margin-top: 0%;">
		<ul style="font-size:34px;">
			<li>Reward: a weighted sum of terms ($r_{pos},r_{vel}, r_{end},r_{root},r_{com}$) that encourge the policy to track a reference motion
			</li>
			<li>Actuation model:
				<ul>
					<li>target joint angles with given $k_p$ and $k_d$;</li>
					<li>target joint velocities with given $k_d$;</li>
					<li>torques;</li>
					<li>muscle activations.</li>
				</ul>
			</li>
			<li>A PD controller can outperform a torque controller in both training speed and final control performance.</li>
			<li>Explanation:
				<ul>
                    <li>Smoother learning curve.</li>
					<li>A PD controller starts as a standing controller while a torque controller doesn't.</li>
				</ul>
			</li>
		</ul>
	</div>
	<div class="rightcol">
		<img src="./figures/Peng2017_PD.png">
	</div>
	
</section>
<section id="slide-anymal-train-curriculum" class="center">
	<h4 id="h4_anymal">Anymal: Training</h4>
    <div class="leftcol" style="margin-top: 0%;">
        <ul style="font-size:35px;">
            <li>Learning algorithm: Trust Region Policy Optimization.
            </li>
            <li>Curriculum: avoid trapping in a local minimum of penalty on joint torque and velocity<br>
                <ul>
                    <li>all of cost terms except those related to the objective (base velocity error cost etc.) are multiplied by a curriculum factor $k_c$, where $k_{c, j+1}\leftarrow (k_{c, j})^{k_d}$ and $k_0, k_d\in (0, 1)$ are selected to prevent the initial tendency to stand still.</li>
                </ul>
            </li>
            <li>The discount factor $\gamma$ is hand-tuned.</li>
            <li>Joint velocities, base velocities: uniformly distributed noise injection</li>
            <li>Training time: 9 days of simulated time for command-conditioned locomotion; 79 days for recovery from a fall.</li>
        </ul>
    </div>
    <div class="leftcol" style="margin-top: 0%;">
        <img src="./figures/anymal_motor.png">
    </div>
</section>
<section id="slide-anymal-reward" class="center">
    <h4 id="h4_anymal">Anymal: Reward Setting</h4>
    <div>
        <ul style="font-size:30px;">
            <li>angular velocity and linear velocity of the base cost
                $$
                    c_wK(|\omega_{IB}^I-\hat\omega_{IB}^I|), c_w=-6\Delta t
                $$
            </li>
            <li>torque cost, joint speed cost
                $$
                    k_cc_\tau\|\tau\|^2, c_\tau=0.005\Delta t
                $$
            </li>
            <li>foot clearance cost and foot slip cost</li>
            <li>orientation cost</li>
            <li>smoothness cost
                $$
                    k_cc_s\|\tau_{t-1}-\tau_t\|^2
                $$
            </li>
            <li>$K:\mathbb{R}\rightarrow [-0.25, 0)$
                $$
                   K(x)=-\frac{1}{e^x+e^{-x}+2}
                $$
            </li>
        </ul>
    </div>
</section>
<section id="slide-anymal-deployment" class="center">
	<h4 id="h4_anymal">Anymal: Deployment</h4>
    <div class="leftcol" style="margin-top: 0%;">
        <ul style="font-size:35px;">
            <li>Inference takes 25$\mu$s on a single CPU thread, less than 0.25% of the available onboard computational resources.</li>
            <li>The policy network is evaluated at 200Hz for command-conditioned locomotion and at 100 Hz for recovery from a fall.</li>
            <li>The performance is insensative to the control rate: performance is identical for recovery when the control rate increases from 20 Hz to 100 Hz. (Possibly due to the low joint velocities mostly below 6 rad/s)
            </li>
        </ul>
    </div>
    <div class="leftcol" style="margin-top: 0%;">
        <img src="./figures/anymal_motor.png">
    </div>
</section>
<section id="slide-anymal_motor-video" class="center">
    <video controls>
        <source src="./videos/anymal_motor.mp4"
                type="video/webm">
    </video>
</section>

<!--Anymal locomotion on challenging terrains-->
<section id="anymal-locomotion" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain<a class="org-ref-reference" href="#slide-bibliography">[Lee2020]</a></h4>
    <ul>
        <li>Goal: follow a command (direction) and locomote over rough terrain with only proprioceptive measurements (joint encoders and an IMU).
        </li>
        <li>Challenge: the robustness of the controller.</li>
    </ul>
</section>
<section id="anymal-locomotion-policy-teacher" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Policy</h4>
    <div class="leftcol">
        <ul style="font-size:35px;">
            <li>Privileged learning</li>
            <li>teacher policy (access to ground-truth knowledge)
                <ul>
                    <li>state $s_t=(o_t, x_t)$</li>
                    <li>$o_t$ inclues command, orientation, base twist, joint positions and velocities, periodic phase $\phi_i$, frequency offset $f_i$ of each leg, previous foot position targets, joint position errors and velocities at $t-0.01$ aand $t-0.02$ s.</li>
                    <li>$x_t$ contains noiseless information from the physics engine, mainly related to foot-ground interactions, such as terrain profile (9 points placed along a 10-cm radius circle), foot contact state and forces, friction coefficients, external disturbance forces.</li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="r-stack">
        <img src="./figures/anymal_loco_architecture.png">
    </div>
</section>
<section id="anymal-locomotion-policy-teacher2" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Policy</h4>
    <div class="leftcol">
        <ul style="font-size:35px;">
            <li>Privileged learning</li>
            <li>teacher policy (access to ground-truth knowledge)
                <ul>
                    <li>action: 16-dim vector (leg frequencies and foot position residuals)</li>
                    <li>architecture: MLP encoder with input $x_t$ and MLP for computing action</li>
                    <li>Learning algorithm: TRPO</li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="r-stack">
        <img src="./figures/anymal_loco_architecture.png">
    </div>
</section>
<section id="anymal-locomotion-policy-student" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Policy</h4>
    <div class="leftcol">
        <ul style="font-size:35px;">
            <li>Privileged learning</li>
            <li>student policy (only uses sensors avaliable on the real robot; imitates the teacher)
                <ul>
                    <li>state $s_t=(o_t)$</li>
                    <li>architecture: a temporal convolutional network (TCN) encoder and an MLP for computing action</li>
                    <li>Learning algorithm: supervised learning with a loss function
                        $$
                            \begin{aligned}
                            L=&(\bar a_t(o_t, x_t)-a_t(o_t, H))^2+\\
                            &(\bar l_t(o_t,x_t)-l_t(H))^2,
                            \end{aligned}
                        $$
                        where $H=\{h_{t-1},\cdots,h_{t-N-1}\}$ and $h_t=o_t\setminus\{f_o, \textrm{joint history, previous foot position targets}\}$ is a time series of proprioceptive observations.
                    </li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="r-stack">
        <img src="./figures/anymal_loco_architecture.png">
    </div>
</section>
<section id="anymal-locomotion-curriculum" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Policy</h4>
    <ul>
        <li>Adaptive terrain curriculum</li>
    </ul>
</section>
<section id="anymal-locomotion" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Training</h4>
    <ul>
        <li>Goal:</li>
    </ul>
</section>
<section id="anymal-locomotion" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Reward Setting</h4>
    <ul>
        <li>Goal:</li>
    </ul>
</section>
<section id="anymal-locomotion" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Deployment</h4>
    <ul>
        <li>Training environment features only rigid terrain, while the controller successfully meet the dversity of field conditions.</li>
    </ul>
</section>
<section id="slide-anymal_locomotion-video" class="center">
    <video controls>
        <source src="./videos/anymal_locomotion.mp4"
                type="video/webm">
    </video>
</section>
<!--Cassie-->
<section id="cassie-traditional" class="center">
    <h4>Bipedal Locomotion: Traditional Controller Design</h4>

</section>
<section id="cassie-gait-learning" class="center">
    <h4>Cassie: Bipedal Gait Learning</h4>
    <ul>
        <li>Goal: learn all common gaits.</li>
        <li>Challenge: the reward function is hard to design
            <ul>
                <li>track trajectory motion: deriving feasible reference trajectories is hard</li>
                <li>reference-free reward: inefficient, infeasible or unsafe policies.</li>
            </ul>
        </li>
    </ul>
</section>
<section id="cassie-periodic-gait" class="center">
    <h4>Cassie: Bipedal Gait Learning</h4>
    <div class="leftcol">
        <ul style="font-size:30px;">
            <li>Gait: a sequence of periodic swing phases and stance phases.</li>
            <li>Bahavior reward: biased sum of $n$ reward components
                $$
                    \begin{aligned}
                        R(s,\phi)&=\sum_i R_i(s, \phi)\\
                        R_i(s,\phi)&=c_i\cdot I_i(\phi)\cdot q_i(s),
                    \end{aligned}
                $$
                where $\phi\in[0, 1]$ denotes the cycle time; $I_i(\phi)$ is a binary-valued random variable denoting whether the target phase is active; $q_i(s)$ is the reward (e.g. norm of a foot force)
            </li>
            <li>$$
                    P(I_i(\phi)=x)=\left\{
                        \begin{aligned}
                            &P(A_i<\phi< B_i) \quad \textrm{if}\ x=1 \\
                            &1-P(A_i<\phi< B_i) \quad \textrm{if}\ x=0 \\
                        \end{aligned}
                        \right.
                $$
                where $A_i\sim \Phi(2\pi a_i,\kappa), B_i\sim\Phi(2\pi b_i,\kappa)$ and $\Phi$ is the Von Mises distribution (approximation of the wrapped Normal distribution).
            </li> 
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_gait_circle.png">
    </div>
</section>
<section id="cassie-periodic-reward" class="center">
    <h4>Cassie: Bipedal Gait Learning</h4>
    <div class="leftcol">
        <ul style="font-size:30px;">
            <li>Reward coefficients:
                $$
                    \begin{aligned}
                    \mathbb{E}[C_{frc}(\phi)]=&c_{swing\ frc}\cdot \mathbb{E}[I_{swing\ frc}(\phi)]+\\
                    &c_{stance\ frc}\cdot \mathbb{E}[I_{stance\ frc}(\phi)]
                    \end{aligned}
                $$
            </li>
            <li>Reward: bipedal bahavior, smooth terms and penalization of control inputs
                $$
                    \mathbb{E}[R(s,\phi)]=\mathbb{E}[R_{bipedal}(s,\phi)]+R_{smooth}(s)+R_{cmd}(s)
                $$
                $$
                    \begin{aligned}
                        \mathbb{E}[R_{bipedal}(s,\phi)]&=\mathbb{E}[C_{frc}(\phi+\theta_{left})]\cdot q_{left\ frc}(s)+\cdots\\
                        &+\mathbb{E}[C_{spd}(\phi+\theta_{left})]\cdot q_{left\ spd}(s)+\cdots,
                    \end{aligned}
                $$
                where $\theta_{left}$ and $\theta_{right}$ define the phases.
            </li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_gait_reward.png">
    </div>
</section>
<section id="slide-cassie-policy" class="center">
    <h4>Cassie: Policy</h4>
    <div class="leftcol">
        <ul>
            <li>Input: 
                <ul>
                    <li>robot state $\hat q, \hat{\dot q}$</li>
                    <li>desired velocity $\dot x_{desired}, \dot y_{desired}$</li>
                    <li>phase ratios and clock inputs $r, p=\{\sin(2\pi(\phi+\theta_{left})), \sin(2\pi(\phi+\theta_{right}))\}$</li>
                </ul>
            </li>
            <li>Output: 10 desired joint positions and 10 sets of PD gains</li>
            <li>Architecture: LSTM with 2 recurrent hidden layers of size 128 each and a simple linear output projection of size 30.</li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_gait_architecture.png">
    </div>
</section>
<section id="slide-cassie-train" class="center">
    <h4>Cassie: Training</h4>
    <div class="leftcol">
        <ul>
            <li>Dynamics randomization.</li>
            <li>Learning algorithm: PPO</li>
            <li>Simulator: cassie-mujoco-sim base on MuJoCo</li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_randomization.png">
    </div>
</section>
<section id="slide-cassie-deployment" class="center">
    <h4>Cassie: Deployment</h4>
    <ul>
        <li>Policy evaluation (40 Hz) and PD tracking (2000 Hz).</li>
        <li></li>
    </ul>
</section>
<section id="slide-cassie-gait-video" class="center">
    <video controls>
        <source src="./videos/cassie_gait.mp4"
                type="video/webm">
    </video>
</section>

<section id="slide-cassie-step-policy" class="center">
    <h4>Cassie: Footstep-Constrained Walking  - Policy</h4>
    <div class="leftcol">
        <ul>
            <li>Input: 
                <ul>
                    <li>rotational velocity and orientation of the base; robot state $\hat q, \hat{\dot q}$</li>
                    <li>clock inputs $p=\{\sin(2\pi(\phi+\theta_{left})), \sin(2\pi(\phi+\theta_{right}))\}$</li>
                    <li>swing-to-stance ratio $r$ and turn rate input </li>
                    <li>desired footstep constraint $U=(L_{step}, \theta_{step})$</li>
                </ul>
            </li>
            <li>Output: 10 desired joint positions</li>
            <li>Architecture: LSTM with 2 recurrent hidden layers of size 128 each and a simple linear output projection of size 30.</li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_step_architecture.png">
    </div>
</section>
<section id="slide-cassie-step-reward" class="center">
    <h4>Cassie: Footstep-Constrained Walking  - Reward</h4>
    <div class="leftcol">
        <ul>
            <li>Reward at two time scales:
                <ul>
                    <li>locomotion: at every decision circle
                        $$
                            r_{pelvis}=100(\Delta d^i-\Delta d^{i-1})
                        $$
                    </li>
                    <li>footstep-constraint: only at touchdown moment
                        $$
                            r_{step}=\exp(-2f),
                        $$
                        where $f$ is the Euclidean distance between the actual and target footstep locations.
                    </li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_step_reward.png">
        <img src="./figures/cassie_step_reach.png">
    </div>
</section>

<section id="slide-cassie-load-policy" class="center">
    <h4>Cassie: Footstep-Constrained Walking  - Reward</h4>
    <div class="leftcol">
        <ul>
            <li></li>
        </ul>
    </div>
    <div class="rightcol">

    </div>
</section>
<section id="slide-cassie-step-video" class="center">
    <video controls>
        <source src="./videos/cassie_step.mp4"
                type="video/webm">
    </video>
</section>
<section id="slide-cassie-load-video" class="center">
    <video controls>
        <source src="./videos/cassie_load.mp4"
                type="video/webm">
    </video>
</section>
<!--Conclusion-->
<section id="slide-conclusion" class="center">
	<h2 id="h2_conclusion" hidden>Conclusion</h2>
	<h3 id="h3_conclusion" hidden>Conclusion</h3>
	<h4 id="conclusion">CONCLUSION</h4>
	<ul>
		<li></li>
	</ul>
	<div class="slide-footer"><br></div>
</section>

<section id="slide-thanks" class="center">
	<div style="font-size:100px;text-align:center;">
		<strong>Thanks!</strong>
	</div>
</section>

<section id="slide-bibliography">
	<h3 id="h3_bibliography" hidden>Bibliography</h3>
	<h4 id="bibliography">Bibliography</h4>
	<ul class='org-ref-bib'>
		<li><a id="Zhao2020">[Zhao2020]</a> <a name="Zhao2020"></a>Zhao, Wenshuai, Jorge Peña Queralta, and Tomi Westerlund. "Sim-to-real transfer in deep reinforcement learning for robotics: a survey." 2020 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE, 2020.</li>
		<li><a id="Peng2017">[Peng2017]</a> <a name="Peng2017"></a>Peng, Xue Bin, and Michiel van de Panne. "Learning locomotion skills using deeprl: Does the choice of action space matter?." Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 2017.</li>
        <li><a id="Lee2020">[Lee2020]</a> <a name="Lee2020"></a>Lee, Joonho, et al. "Learning quadrupedal locomotion over challenging terrain." Science robotics 5.47 (2020): eabc5986</li>
        <li><a id="Hurst2021">[Hurst2021]</a> <a name="Hurst2021"></a>Siekmann, Jonah, et al. "Sim-to-real learning of all common bipedal gaits via periodic reward composition." 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021</li>
        <li><a id="Duan2022">[Duan2022]</a> <a name="Hurst2021"></a>Duan, Helei, et al. "Sim-to-Real Learning of Footstep-Constrained Bipedal Dynamic Walking." arXiv preprint arXiv:2203.07589 (2022)</li>
        <li><a id="Dao2022">[Dao2022]</a> <a name="Dao2022"></a>J. Dao, K. Green, H. Duan, A. Fern and J. Hurst, "Sim-to-Real Learning for Bipedal Locomotion Under Unsensed Dynamic Loads," 2022 International Conference on Robotics and Automation (ICRA), 2022, pp. 10449-10455</li>
	</ul>
	<div class="slide-footer"><br></div>
</section>
</section>



<style type="text/css"> div.flushright{text-align:right;} </style>
<script src="./reveal.js/plugin/chart/Chart.min.js"></script>
<script src="./reveal.js/plugin/chart/plugin.js"></script>
<script src="./reveal.js/plugin/mine/chartjs-plugin-colorschemes.js"></script>
<script src="./reveal.js/plugin/mine/bokeh-2.2.2.min.js"></script>
<script src="./reveal.js/plugin/mine/bokeh-widgets-2.2.2.min.js"></script>
<script src="./reveal.js/plugin/mine/bokeh-tables-2.2.2.min.js"></script>
<script type="text/javascript" sync src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js"></script>

<script src="./reveal.js/dist/reveal.js"></script>
<script src="./reveal.js/plugin/notes/notes.js"></script>
<script src="./reveal.js/plugin/search/search.js"></script>
<script src="./reveal.js/plugin/zoom/zoom.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration

Reveal.initialize({
controls: true,
progress: true,
history: true,
center: false,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: true,
fragmentInURL: true,
hashOneBasedIndex: false,
pdfSeparateFragments: false,

overview: true,

transition: 'fade',
transitionSpeed: 'default',
margin: 0.01,


// Optional libraries used to extend reveal.js
dependencies: [
{src:'./reveal.js/plugin/spotlight/spotlight.js'},
{src: './reveal.js/plugin/math/math-gc.js', async:true},
{src: './reveal.js/plugin/toc-progress/toc-progress.js', async: true, callback: function() { toc_progress.initialize('reduce', 'rgba(120,138,130,0.2)', 'body'); }}
],

chart: {
  defaults: {
	  	global:{
			defaultFontSize: 20,
			title:{
				display: true,
				fontColor: "black",
				fontSize: 40
			},
			scales: {
				scaleLabel: { fontColor: "black"}, 
				gridLines: { color: "#FFF", zeroLineColor: "#FFF" }
			} ,
			legend:{
					display: true,
					labels: {fontColor: "black", fontSize: 30 }
			}
		}
	}
},

plugins: [RevealNotes, RevealSearch, RevealZoom, RevealChart], spotlight: {toggleSpotlightOnMouseDown: false}, keyboard: {84: function() { RevealSpotlight.toggleSpotlight() }}, width: 1366, height: 768, margin: 0.1,
audioStartAtFragment: true,
  audio: {
    advance: -1, autoplay: false, defaultDuration: 0, defaultAudios: false, playerOpacity: 0.8, playerStyle: 'position: fixed; bottom: 9.5vh; left: 0%; width: 30%; height:30px; z-index: 33;' },
anything: [
        // Following initialization code for class animate from anything-demo.html.
        // Copyright (c) 2016 Asvin Goel, under The MIT License (MIT).
	{className: "animate",  initialize: (function(container, options){
		Reveal.addEventListener( 'fragmentshown', function( event ) {
			if (typeof event.fragment.beginElement === "function" ) {
				event.fragment.beginElement();
			}
		});
		Reveal.addEventListener( 'fragmenthidden', function( event ) {
			if (event.fragment.hasAttribute('data-reverse') ) {
				var reverse = event.fragment.parentElement.querySelector('[id=\"' + event.fragment.getAttribute('data-reverse') + '\"]');
				if ( reverse && typeof reverse.beginElement === "function" ) {
					reverse.beginElement();
				}
			}
		});
		if ( container.getAttribute("data-svg-src") ) {
			var xhr = new XMLHttpRequest();
			xhr.onload = function() {
				if (xhr.readyState === 4) {
					var svg = container.querySelector('svg');
					container.removeChild( svg );
					container.innerHTML = xhr.responseText + container.innerHTML;
					if ( svg ) {
						container.querySelector('svg').innerHTML = container.querySelector('svg').innerHTML + svg.innerHTML;
					}
				}
				else {
					console.warn( "Failed to get file. ReadyState: " + xhr.readyState + ", Status: " + xhr.status);
				}
			};
			xhr.open( 'GET', container.getAttribute("data-svg-src"), true );
			xhr.send();
		}
	}) },
	{className: "randomPic",
	 defaults: {imgalt: "Dummy alt text",
		    imgcaption: "Image by {name}",
		    choices: [ {name: "dummyname", path: "dummypath"} ]},
	 initialize: (function(container, options){
	     var choice = Math.trunc( Math.random()*(options.choices.length) );
	     var img = "<img src='" + options.choices[choice].path + "' alt='" + options.choices[choice].imgalt + "' />";
	     var caption = options.imgcaption.replace(new RegExp('\{name\}', 'gm'), options.choices[choice].name);
	     container.innerHTML = img + caption;
	 }) },
	{className: "notes",
	 initialize: (function(container, options){
	     container.addEventListener('click', function(e) { Reveal.getPlugins().notes.open(); });
	 }) }
],
coursemod: { enabled: true, shown: false },
});
</script>

<script>
Reveal.configure({
  keyboard: {
    39: 'next', // go to the next slide when the ENTER key is pressed
    37: 'prev' // don't do anything when SPACE is pressed (i.e. disable a reveal.js default binding)
  }
})
</script>
</body>
</html>

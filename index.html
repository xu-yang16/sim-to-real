<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Sim-to-Real Learning for Legged Locomotion</title>
<meta name="author" content="Xu Yang"/>
<meta name="description" content="group meeting on 2022.09.28"/>
<meta name="keywords" content="RL; sim-to-real; legged locomotion"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./reveal.js/dist/reveal.css"/>
<link rel="stylesheet" href="./reveal.js/dist/theme/oer-reveal.css" id="theme"/>
<link rel="stylesheet" href="./reveal.js/plugin/accessibility/helper.css"/>
<link rel="stylesheet" href="./reveal.js/plugin/toc-progress/toc-progress.css"/>
<link rel="stylesheet" href="./reveal.js/dist/theme/toc-style.css"/>
<link rel="stylesheet" href="./reveal.js/dist/theme/fonts/source-sans-pro/source-sans-pro.css"/>
<link rel="stylesheet" href="./reveal.js/plugin/mine/font-awesome.min.css">
</head>
<body prefix="dc: http://purl.org/dc/elements/1.1/ dcterms: http://purl.org/dc/terms/ dcmitype: http://purl.org/dc/dcmitype/ cc: http://creativecommons.org/ns#" typeof="dcmitype:InteractiveResource">
	<style>
		:root{
			--myred: #981E32;
		}
		:root{
			--myblue: #0582CA;
		}
	</style>
<div class="reveal">
<div class="slides">

<section>
<section id="sec-title-slide" data-state="no-toc-progress"  class="center">
<!-- The following copyright information only applies to the title slide. -->
<!-- SPDX-FileCopyrightText: 2017-2019 Jens Lechtenbörger -->
<!-- SPDX-License-Identifier: CC0-1.0 -->
<!-- This is an HTML template for the title slide. -->
<!-- Embed logos as necessary. -->
<!-- <a class="nooutlink" href="url"><img class="state-background your-logo-class" src="whatever.png" alt="Whatever" /></a> -->

<div class="talk-title">
    <h1 class="no-toc-progress" style="text-align:center;">Sim-to-Real Learning for Legged Locomotion</h1>
</div>
<div class="talk-subtitle">
    <p></p>
</div>
<div class="talk-author">
  	<p style="font-size:55px;text-align:center;">
	  	Xu Yang
	</p>
	<p style="font-size:35px;text-align:center;">
		<strong style="color:black;">Department of Automation, Tsinghua University<br></br></strong>
	</p>
</div>
</section>

<section id="slide-content" class="center">
	<h3 id="content">Table of Contents</h3>
	<div id="text-table-of-contents">
	<ul>
		<li><a href="#slide-intro">Introduction</a></li>
		<li><a href="#slide-anymal-traditional">Case study: Anymal</a></li>
		<li><a href="#slide-cassie-traditional">Case study: Cassie</a></li>
		<li><a href="#slide-conclusion">Conclusion</a></li>
	</ul>
	</div>
	
	<div class="slide-footer"><br></div>
</section>

<!--Background-->
<section id="slide-intro" class="center">
	<h3 id="h3_intro" hidden>Introduction</h3>
	<h4 id="h4_intro">Introduction</h4>
    <div>
        <ul style="font-size:40px;">
            <li><strong><em style="color:var(--myred);">Reward function</em></strong>.<br>
                How to design reward functions to obtain expected controller performance?
            </li>
            <li><strong><em style="color:var(--myred);">Input and output</em></strong>.<br>
                What can you measure and what can you control?
            </li>
            <li><strong><em style="color:var(--myred);">Modelling</em></strong>.<br>
                simuation fidelity (rigid-body dynamics and actuator dynamics); dynamics randomization
            </li>
            <li><strong><em style="color:var(--myred);">Training</em></strong>.<br>
                parallel differentiable simulation; curriculum; teacher and student
            </li>
            <li><strong><em style="color:var(--myred);">Learning algorithm</em></strong>.<br>
                PPO etc.
            </li>
            <li><strong><em style="color:var(--myred);">Network design</em></strong>.<br>

            </li>
        </ul>
    </div>
</section>
<section id="slide-intro-table" class="center" data-background-image="./figures/review.png">
    <!-- <h4 id="h4_intro">Introduction</h4> -->
    <!-- <div class="r-stack">
        <img src="./figures/review.png" width="1000" height="1000">
    </div> -->
</section>
<!--Anymal-->
<section id="slide-anymal-traditional" class="center">
	<h3 id="h3_anymal" hidden>Case study: Anymal</h3>
	<h4 id="h4_anymal">Qradruped Robot: Traditional Controller Design</h4>
    <div>
        <ul style="font-size:40px;">
            <li>Goal: valid gait patterns (contact sequences) and commanded velocity tracking.</li>
            <li>Controller: <strong><em style="color:var(--myred);">planning</em></strong> and <strong><em style="color:var(--myred);">tracking</em></strong>.<br>
                
            </li>
            <li>Trajectory optimization for a <strong><em style="color:var(--myred);">complex rigid-body model</em></strong> with many unspecified <strong><em style="color:var(--myred);">contact points</em></strong> is beyond the capabilities of current optimization techniques. <strong><em style="color:var(--myblue);">Approximations</em></strong> are needed to reduce complexity.
                <ul>
                    <li>Rigid-body model: point mass, whole body...</li>
                    <li>Contact points: smooth contact dynamics...</li>
                </ul>
            </li>
        </ul>
    </div>
</section>
<section id="slide-anymal-motor-idea" class="center">
	<h4 id="h4_anymal">Anymal: Learning Motor Skills<a class="org-ref-reference" href="#slide-bibliography">[Hwangbo2019]</a></h4>
    <div class="leftcol" style="margin-top: 0%;">
        <ul style="font-size:35px;">
            <li>Goal: commanded velocity tracking and recovery from a fall.</li>
            <li>Modelling<a class="org-ref-reference" href="#slide-bibliography">[Raisim]</a>: rigid-body dynamics algorithms $O(n)$ + collision-detection library $O(n^2)$ + contact solver ($O(n)$ if not coupled) + <strong><em style="color:var(--myred);">actuator dynamics</em></strong>
            <li>Challenge: motors (SEAs) are hard to model.
            </li>
        </ul>
    </div>
    <div class="leftcol" style="margin-top: 0%;">
        <img src="./figures/anymal_motor_idea.png" >
    </div>
</section>
<section id="slide-anymal-motor" class="center">
	<h4 id="h4_anymal">Anymal: Control and State Estimation of Motors</h4>
    <div class="leftcol" style="margin-top: 0%;">
        <ul style="font-size:34px;">
            <li>Actuator network: 
                <ul>
                    <li>input: <strong><em style="color:var(--myred);">joint state history</em></strong> (including joint velocity and joint position error), which consists of the current state and 2 past states that correspond to $t-0.01$ and $t-0.02$ s.</li>
                    <li>the length of the history should be sufficiently longer than the sum of all communication delays and the mechanical responce time.</li>
                    <li>output: <strong><em style="color:var(--myred);">joint torque</em></strong></li>
                    <li>architecture: an MLP with 3 hidden layers of 32 units each; softsign activation function (12.2 $\mu$s) instead of tanh (31.6$\mu$s).</li>
                </ul>
            </li>
            
            <li>Dataset: the excitation must cover a wide range of frequency spectra.
                <ul>
                    <li>Foot trajectories in the form of a sine wavewith varied amplitude (5 to 10 cm) and frequency (1 to 25 Hz).</li>
                    <li>Constant contact with the ground.</li>
                    <li>The robot is disturbed manually.</li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="leftcol" style="margin-top: 0%;">
        <img src="./figures/anymal_motor_diagram.png" width="450" height="700">
    </div>
</section>
<section id="slide-anymal-torque" class="center">
	<h4 id="h4_anymal">Anymal: Control and State Estimation of Motors</h4>
    <div class="r-stack">
        <img src="./figures/anymal_motor_torque.png">
    </div>
</section>
<section id="slide-anymal-policy" class="center">
	<h4 id="h4_anymal">Anymal: Policy </h4>
    <div class="leftcol" style="margin-top: 0%;">
        <ul style="font-size:35px;">
            <li>Hyrbid simulator: 
                <ul>
                    <li>nearly 50 000 time steps per second (half of the run time is used to evaluate the actuator nets)</li>
                </ul>
            </li>
            <li>Policy network:
                <ul>
                    <li>input: <strong><em style="color:var(--myred);">$(o_k=<\Phi^g, r_z, v, \omega, \Phi, \dot\Phi, \Theta, a_{k-1}, C>)$</em></strong></li>
					<ul>
						<li>Body twists $\Phi^g$ (direction of the gravity vector in the IMU frame); </li>
						<li>height, linear and angular veclocities of the base; joint angles, velocities; </li>
						<li>joint state history $\Theta$ sparsely sampled at $t_k-0.01$ s to $t_k-0.002$ s; (enabling contact detection, which could be replaced by force sensors)</li>
						<li>previous action; command $C$.</li>
					</ul>
                    <li>output: <strong><em style="color:var(--myred);">joint position target</em></strong> (why not torques or velocities?)</li>
                    <li>architecture: an MLP with two hidden layers, with 256 and 128 units each and tanh nonlinearity (bounded activation functions yield less aggressive trajectories for real robots when subjected disturbances)</li>
            </li>
        </ul>
    </div>
    <div class="rightcol" style="margin-top: 0%;">
        <img src="./figures/anymal_motor_diagram.png" width="450" height="700">
    </div>
</section>
<section id="slide-Peng2017" class="center">
	<h4 id="h4_anymal">Learning Locomotion Skills: Does the Choice of Action Space Matter?<a class="org-ref-reference" href="#slide-bibliography">[Peng2017]</a></h4>
    <div class="leftcol" style="margin-top: 0%;">
		<ul style="font-size:34px;">
			<li>Reward: a weighted sum of terms ($r_{pos},r_{vel}, r_{end},r_{root},r_{com}$) that encourge the policy to track a reference motion
			</li>
			<li>Actuation model:
				<ul>
					<li><strong><em style="color:var(--myred);">target joint angles</em></strong> with given $k_p$ and $k_d$;</li>
					<li><strong><em style="color:var(--myred);">target joint velocities</em></strong> with given $k_d$;</li>
					<li><strong><em style="color:var(--myred);">torques</em></strong>;</li>
					<li>muscle activations.</li>
				</ul>
			</li>
			<li>A PD controller can outperform a torque controller in both <strong><em style="color:var(--myred);">training speed</em></strong> and <strong><em style="color:var(--myred);">final control performance</em></strong>.</li>
			<li>Explanation:
				<ul>
                    <li>Smoother learning curve.</li>
					<li>A PD controller starts as a standing controller while a torque controller doesn't.</li>
				</ul>
			</li>
		</ul>
	</div>
	<div class="rightcol">
		<img src="./figures/Peng2017_PD.png">
	</div>
	
</section>
<section id="slide-anymal-train-curriculum" class="center">
	<h4 id="h4_anymal">Anymal: Training</h4>
    <div class="leftcol" style="margin-top: 0%;">
        <ul style="font-size:35px;">
            <li>Learning algorithm: Trust Region Policy Optimization.
            </li>
            <li>Curriculum: avoid trapping in a local minimum of penalty on joint torque and velocity<br>
                <ul>
                    <li>all of cost terms except those related to the objective (base velocity error cost etc.) are <strong><em style="color:var(--myred);">multiplied by a curriculum factor $k_c$</em></strong>, where $k_{c, j+1}\leftarrow (k_{c, j})^{k_d}$ and $k_0, k_d\in (0, 1)$ are selected to prevent the initial tendency to stand still.</li>
                </ul>
            </li>
            <li>The discount factor $\gamma$ is hand-tuned.</li>
            <li>Joint velocities, base velocities: uniformly distributed <strong><em style="color:var(--myred);">noise injection</em></strong></li>
            <li>Training time: 9 days of simulated time for command-conditioned locomotion; 79 days for recovery from a fall.</li>
        </ul>
    </div>
    <div class="leftcol" style="margin-top: 0%;">
        <img src="./figures/anymal_motor_tracking.png">
    </div>
</section>
<section id="slide-anymal-reward" class="center">
    <h4 id="h4_anymal">Anymal: Reward Setting</h4>
    <div>
        <ul style="font-size:30px;">
            <li>angular velocity and linear velocity of the base cost
                $$
                    c_wK(|\omega_{IB}^I-\hat\omega_{IB}^I|), c_w=-6\Delta t
                $$
            </li>
            <li>torque cost, joint speed cost
                $$
                    k_cc_\tau\|\tau\|^2, c_\tau=0.005\Delta t
                $$
            </li>
            <li>foot clearance cost and foot slip cost</li>
            <li>orientation cost</li>
            <li>smoothness cost
                $$
                    k_cc_s\|\tau_{t-1}-\tau_t\|^2
                $$
            </li>
            <li>$K:\mathbb{R}\rightarrow [-0.25, 0)$
                $$
                   K(x)=-\frac{1}{e^x+e^{-x}+2}
                $$
            </li>
        </ul>
    </div>
</section>
<section id="slide-anymal-deployment" class="center">
	<h4 id="h4_anymal">Anymal: Deployment</h4>
    <div class="leftcol" style="margin-top: 0%;">
        <ul style="font-size:35px;">
            <li>Inference takes 25$\mu$s <strong><em style="color:var(--myred);">on a single CPU thread</em></strong> (Intel i7-5600U, 2.6–3.2GHz, dual-core 64-bit, Tensorflow C++ API is used), less than 0.25% of the available onboard computational resources.</li>
            <li>The policy network is evaluated at 200Hz for command-conditioned locomotion and at 100 Hz for recovery from a fall.</li>
            <li>The performance is <strong><em style="color:var(--myred);">insensative to the control rate</em></strong>: performance is identical for recovery when the control rate increases from 20 Hz to 100 Hz. (Possibly due to the low joint velocities mostly below 6 rad/s)
            </li>
        </ul>
    </div>
    <div class="leftcol" style="margin-top: 0%;">
        <img src="./figures/anymal_motor.png">
    </div>
</section>
<section id="slide-anymal_motor-video" class="center">
    <video controls>
        <source src="./videos/anymal_motor.mp4"
                type="video/webm">
    </video>
</section>

<!--Anymal locomotion on challenging terrains-->
<section id="anymal-locomotion" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain<a class="org-ref-reference" href="#slide-bibliography">[Lee2020]</a></h4>
    <div class="leftcol">
        <ul>
            <li>Goal: follow a command (<strong><em style="color:var(--myred);">direction</em></strong>) and locomote over rough terrain with <strong><em style="color:var(--myred);">only proprioceptive measurements</em></strong> (joint encoders and an IMU).
            </li>
            <li>Challenge: the robustness of the controller.</li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/anymal_loco_challenge.png">
    </div>
</section>
<section id="anymal-locomotion-policy-teacher" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Policy</h4>
    <div class="leftcol">
        <ul style="font-size:35px;">
            <li>Privileged learning</li>
            <li><strong><em style="color:var(--myred);">teacher policy</em></strong> (access to ground-truth knowledge)
                <ul>
                    <li>state $s_t=(o_t, x_t)$</li>
                    <li>$o_t$ includes command, orientation, base twist, joint positions and velocities, periodic phase $\phi_i$, frequency offset $f_i$ of each leg, previous foot position targets, joint position errors and velocities at $t-0.01$ and $t-0.02$ s.</li>
                    <li>$x_t$ contains noiseless information from the physics engine, mainly related to foot-ground interactions, such as terrain profile (9 points placed along a 10-cm radius circle), foot contact state and forces, friction coefficients, external disturbance forces.</li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="r-stack">
        <img src="./figures/anymal_loco_architecture.jpg">
    </div>
</section>
<section id="anymal-locomotion-policy-teacher2" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Policy</h4>
    <div class="leftcol">
        <ul style="font-size:35px;">
            <li>Privileged learning</li>
            <li>teacher policy (access to ground-truth knowledge)
                <ul>
                    <li>action: <strong><em style="color:var(--myred);">16-dim vector (leg frequencies $f_i$ and foot position residuals $\Delta r_{fi, T}$)</em></strong>
                        <ul>
                            <li>$\phi_i=\phi_{i,0}+(f_0+f_i)t$: contact phase if $\phi\in[0, \pi)$ and swing phase if $\phi\in[\pi, 2\pi)$, where $f_0=1.25$ Hz.</li>
                            <li>foot trajectory generator: $F(\phi):[0, 2\pi)\rightarrow \mathbb{R}^3$</li>
                            <li>target foot position $F(\phi_i)+\Delta r_{fi,T}$</li>
                            <li>tracking: <strong><em style="color:var(--myred);">analytic IK and PD controller</em></strong></li>
                        </ul>
                    </li>
                    <li>architecture: MLP encoder with input $x_t$ and MLP for computing action</li>
                    <li>Learning algorithm: TRPO</li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="r-stack">
        <img src="./figures/anymal_loco_architecture.jpg">
    </div>
</section>
<section id="anymal-locomotion-policy-student" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Policy</h4>
    <div class="leftcol">
        <ul style="font-size:35px;">
            <li>Privileged learning</li>
            <li>student policy (only uses <strong><em style="color:var(--myred);">sensors avaliable on the real robot</em></strong>; <strong><em style="color:var(--myred);">imitates the teacher</em></strong>)
                <ul>
                    <li>state $s_t=(o_t)$</li>
                    <li>architecture: a temporal convolutional network (TCN) encoder and an MLP for computing action</li>
                    <li>Learning algorithm: supervised learning with a loss function
                        $$
                            \begin{aligned}
                            L=&{\color{var(--myred)}(\bar a_t(o_t, x_t)-a_t(o_t, H))^2}+\\
                            &{\color{var(--myblue)}(\bar l_t(o_t,x_t)-l_t(H))^2},
                            \end{aligned}
                        $$
                        where $H=\{h_{t-1},\cdots,h_{t-N-1}\}$ and $h_t=o_t\setminus\{f_0, \textrm{joint history, previous foot position targets}\}$ is a time series of proprioceptive observations.
                    </li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="r-stack">
        <img src="./figures/anymal_loco_architecture.jpg">
    </div>
</section>
<section id="anymal-locomotion-curriculum" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Policy</h4>
    <div class="leftcol">
        <ul>
            <li><strong><em style="color:var(--myred);">Adaptive terrain curriculum</em></strong>
                <ul>
                    <li>The terrain is generated by a vector $c_T$.</li>
                    <li>Evaluate by the traversability of generated terrains
                        $$
                            \begin{aligned}
                            &v(s_t, a_t, s_{t+1})=1, \textrm{if}\ v_{pr}(s_{t+1})>0.2,\\
                            &Tr(c_T,\pi)=\mathbb{E}_{\xi\sim\pi}\{v(s_t,a_t,s_{t+1}|c_T\}\in[0, 1],\\
                            &Td(c_T,\pi)\triangleq \textrm{Pr}(Tr(c_T,\pi)\in[0.5, 0.9])
                            \end{aligned}
                        $$
                    </li>
                    <li>Sequential importance resampling particle filter</li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/anymal_loco_architecture.jpg">
    </div>
</section>
<section id="anymal-locomotion-policy-performance" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Policy</h4>
    <div class="r-stack">
        <img src="./figures/anymal_loco_policy.jpg">
    </div>
</section>
<section id="anymal-locomotion-reward" class="center">
    <h4 id="h4_anymal">Anymal - Learning Locomotion over Challenging Terrain: Reward Setting and Training</h4>
    <ul>
        <li>Linear velocity; angular velocity; base motion; foot clearance; body collision; smoothness of target foot positions; torque penalty
            $$
                0.05r_{lv}+0.05r_{av}+0.04r_b+0.01r_{fc}+0.02r_{bc}+0.025r_s+2\cdot 10^{-5}r_\tau,
            $$
        </li>
        <li>Training: <strong><em style="color:var(--myred);">12 hours for teacher policy</em></strong> and <strong><em style="color:var(--myblue);">4 hours for student policy</em></strong> (i7-8700K CPU and RTX 2080 GPU)</li>
        <li>Training environment features only <strong><em style="color:var(--myred);">rigid terrain</em></strong>, while the controller successfully meet the <strong><em style="color:var(--myblue);">diversity of field conditions</em></strong>.</li>
    </ul>
</section>
<section id="slide-anymal_locomotion-video" class="center">
    <video controls>
        <source src="./videos/anymal_locomotion.mp4"
                type="video/webm">
    </video>
</section>

<section id="cheetah-locomotion-reward" class="center">
    <h4 id="h4_cheetah">Mini Cheetah</h4>
    <div class="r-stack">
        <img src="./figures/cheetah_reward.png">
    </div>
</section>
<section id="slide-anymal_locomotion-video" class="center">
    <video controls>
        <source src="./videos/cheetah.mp4"
                type="video/webm">
    </video>
</section>
<!--Cassie-->
<section id="slide-cassie-gait-learning" class="center">
    <h4>Cassie: Bipedal Gait Learning</h4>
    <div class="leftcol">
        <ul>
            <li>Goal: <strong><em style="color:var(--myred);">learn all common gaits</em></strong>.</li>
            <li>Challenge: the reward function is hard to design
                <ul>
                    <li><strong><em style="color:var(--myblue);">track trajectory motion</em></strong>: deriving feasible reference trajectories is hard</li>
                    <li><strong><em style="color:var(--myblue);">reference-free reward</em></strong>: inefficient, infeasible or unsafe policies.</li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_robot.png">
    </div>
</section>
<section id="slide-cassie-periodic-gait" class="center">
    <h4>Cassie: Bipedal Gait Learning</h4>
    <div class="leftcol">
        <ul style="font-size:28px;">
            <li>Gait: a sequence of periodic <strong><em style="color:var(--myblue);">swing phases</em></strong> and <strong><em style="color:var(--myblue);">stance phases</em></strong>.</li>
            <li>Bahavior reward: biased sum of $n$ reward components
                $$
                    \begin{aligned}
                        R(s,\phi)&=\sum_i R_i(s, \phi)\\
                        R_i(s,\phi)&=c_i\cdot I_i(\phi)\cdot q_i(s),
                    \end{aligned}
                $$
                where $\phi\in[0, 1]$ denotes the cycle time; $I_i(\phi)$ is a binary-valued random variable denoting whether the target phase is active; $q_i(s)$ is the reward (e.g. norm of a foot force)
            </li>
            <li>$$
                    P(I_i(\phi)=x)=\left\{
                        \begin{aligned}
                            &P(A_i<\phi< B_i) \quad \textrm{if}\ x=1 \\
                            &1-P(A_i<\phi< B_i) \quad \textrm{if}\ x=0 \\
                        \end{aligned}
                        \right.
                $$
                where $A_i\sim \Phi(2\pi a_i,\kappa), B_i\sim\Phi(2\pi b_i,\kappa)$ and $\Phi$ is the Von Mises distribution (approximation of the wrapped Normal distribution).
            </li> 
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_gait_circle.png">
    </div>
</section>
<section id="slide-cassie-periodic-reward" class="center">
    <h4>Cassie: Bipedal Gait Learning</h4>
    <div class="leftcol">
        <ul style="font-size:30px;">
            <li>Reward coefficients:
                $$
                    \begin{aligned}
                    \mathbb{E}[C_{frc}(\phi)]=&c_{swing\ frc}\cdot \mathbb{E}[I_{swing\ frc}(\phi)]+\\
                    &c_{stance\ frc}\cdot \mathbb{E}[I_{stance\ frc}(\phi)]
                    \end{aligned}
                $$
            </li>
            <li>Reward: <strong><em style="color:var(--myblue);">bipedal bahavior, smooth terms and penalization of control inputs</em></strong>
                $$
                    \mathbb{E}[R(s,\phi)]=\mathbb{E}[R_{bipedal}(s,\phi)]+R_{smooth}(s)+R_{cmd}(s)
                $$
                $$
                    \begin{aligned}
                        \mathbb{E}[R_{bipedal}(s,\phi)]&=\mathbb{E}[C_{frc}(\phi+\theta_{left})]\cdot q_{left\ frc}(s)+\cdots\\
                        &+\mathbb{E}[C_{spd}(\phi+\theta_{left})]\cdot q_{left\ spd}(s)+\cdots,
                    \end{aligned}
                $$
                where $\theta_{left}$ and $\theta_{right}$ define the phases.
            </li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_gait_reward.png">
    </div>
</section>
<section id="slide-cassie-periodic-reward2" class="center">
    <h4>Cassie: Bipedal Gait Learning</h4>
    <div class="leftcol">
        <ul style="font-size:30px;">
            <li>Keep the cycle offsets fixed and vary the phase ratio: transition from walking to running and hopping with more or less time in the air.
            </li>
            <li>Vary both the cycle offsets and the phase ratio: introduce additional reward (behavior-specific transition penalities).
                <ul>
                    <li>Hopping: commanded when cycle offsets are very close; cost for constraining the feet positions to be close
                        $$
                            q_{hop\ sym}(s)=1-\exp(-err_{sym}\exp(-5|\sin(2\pi(\theta_{left}-\theta_{right})|)).
                        $$
                    </li>
                    <li>
                        $$
                            q_{standing\ cost}(s)=1-\exp(-(err_{sym}+20q_{action\ diff}(s))).
                        $$
                    </li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_reward.png">
    </div>
</section>
<section id="slide-cassie-policy" class="center">
    <h4>Cassie: Policy</h4>
    <div class="leftcol">
        <ul>
            <li>Input: 
                <ul>
                    <li>robot state $\hat q, \hat{\dot q}$</li>
                    <li>desired velocity $\dot x_{desired}, \dot y_{desired}$</li>
                    <li>phase ratios and clock inputs $r, p=\{\sin(2\pi(\phi+\theta_{left})), \sin(2\pi(\phi+\theta_{right}))\}$</li>
                </ul>
            </li>
            <li>Output: <strong><em style="color:var(--myblue);">10 desired joint positions</em></strong> and <strong><em style="color:var(--myblue);">10 sets of PD gains</em></strong></li>
            <li>Architecture: LSTM with 2 recurrent hidden layers of size 128 each and a simple linear output projection of size 30.</li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_gait_architecture.png">
    </div>
</section>
<section id="slide-cassie-train" class="center">
    <h4>Cassie: Training and Deployment</h4>
    <div class="leftcol">
        <ul>
            <li>Dynamics randomization.</li>
            <li>Learning algorithm: PPO</li>
            <li>Simulator: cassie-mujoco-sim based on MuJoCo</li>
            <li>Policy evaluation (40 Hz) and PD tracking (2000 Hz).</li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_randomization.png">
    </div>
</section>
<section id="slide-cassie-gait-video" class="center">
    <video controls>
        <source src="./videos/cassie_gait.mp4"
                type="video/webm">
    </video>
</section>

<section id="slide-cassie-step-policy" class="center">
    <h4>Cassie: Footstep-Constrained Walking  - Policy</h4>
    <div class="leftcol">
        <ul>
            <li>Input: 
                <ul>
                    <li>rotational velocity and orientation of the base; robot state $\hat q, \hat{\dot q}$</li>
                    <li>clock inputs $p=\{\sin(2\pi(\phi+\theta_{left})), \sin(2\pi(\phi+\theta_{right}))\}$</li>
                    <li>swing-to-stance ratio $r$ and turn rate input </li>
                    <li><strong><em style="color:var(--myblue);">desired footstep constraint</em></strong> $U=(L_{step}, \theta_{step})$</li>
                </ul>
            </li>
            <li>Output: 10 desired joint positions</li>
            <li>Architecture: LSTM with 2 recurrent hidden layers of size 128 each and a simple linear output projection of size 30.</li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_step_architecture.png">
    </div>
</section>
<section id="slide-cassie-step-reward" class="center">
    <h4>Cassie: Footstep-Constrained Walking  - Reward</h4>
    <div class="leftcol">
        <ul>
            <li>Reward at two time scales:
                <ul>
                    <li>locomotion: <strong><em style="color:var(--myred);">at every decision circle</em></strong>
                        $$
                            r_{pelvis}=100(\Delta d^i-\Delta d^{i-1})
                        $$
                    </li>
                    <li>footstep-constraint: <strong><em style="color:var(--myblue);">only at touchdown moment</em></strong>
                        $$
                            r_{step}=\exp(-2f),
                        $$
                        where $f$ is the Euclidean distance between the actual and target footstep locations.
                    </li>
                </ul>
            </li>
        </ul>
    </div>
    <div class="rightcol">
        <img src="./figures/cassie_step_reward.png">
        <img src="./figures/cassie_step_reach.png">
    </div>
</section>

<section id="slide-cassie-step-video" class="center">
    <video controls>
        <source src="./videos/cassie_step.mp4"
                type="video/webm">
    </video>
</section>
<section id="slide-cassie-load-video" class="center">
    <video controls>
        <source src="./videos/cassie_load.mp4"
                type="video/webm">
    </video>
</section>
<!--Conclusion-->
<section id="slide-conclusion" class="center">
	<h2 id="h2_conclusion" hidden>Conclusion</h2>
	<h3 id="h3_conclusion" hidden>Conclusion</h3>
	<h4 id="conclusion">CONCLUSION</h4>
	<ul style="font-size:40px;">
        <li><strong><em style="color:var(--myred);">Reward function</em></strong>.<br>
            Characterization without loss of generality: exploitation of periodicity
        </li>
        <li><strong><em style="color:var(--myred);">Input and output</em></strong>.<br>
            Task space or joint space; position, velocity or torque command?
        </li>
        <li><strong><em style="color:var(--myred);">Modelling</em></strong>.<br>
            simuation fidelity (neural networks for motors); dynamics randomization; manual noise injection
        </li>
        <li><strong><em style="color:var(--myred);">Training</em></strong>.<br>
            curriculum (naive version or adaptive version); teacher and student; parallel differentiable simulation <a class="org-ref-reference" href="#slide-bibliography">[Rudin2021, Margolis2022]</a>
        </li>
        <li><strong><em style="color:var(--myred);">Learning algorithm</em></strong>.<br>
            PPO etc.
        </li>
        <li><strong><em style="color:var(--myred);">Network design</em></strong>.<br>

        </li>
    </ul>
	<div class="slide-footer"><br></div>
</section>

<section id="slide-thanks" class="center">
	<div style="font-size:100px;text-align:center;">
		<strong>Thanks!</strong>
	</div>
</section>

<section id="slide-bibliography">
	<h3 id="h3_bibliography" hidden>Bibliography</h3>
	<h4 id="bibliography">Bibliography</h4>
	<ul class='org-ref-bib'>
		<li><a id="Zhao2020">[Zhao2020]</a> <a name="Zhao2020"></a>Zhao, Wenshuai, Jorge Peña Queralta, and Tomi Westerlund. "Sim-to-real transfer in deep reinforcement learning for robotics: a survey." 2020 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE, 2020.</li>
		<li><a id="Peng2017">[Peng2017]</a> <a name="Peng2017"></a>Peng, Xue Bin, and Michiel van de Panne. "Learning locomotion skills using deeprl: Does the choice of action space matter?." Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 2017.</li>
        <li><a id="Lee2020">[Lee2020]</a> <a name="Lee2020"></a>Lee, Joonho, et al. "Learning quadrupedal locomotion over challenging terrain." Science robotics 5.47 (2020): eabc5986</li>
        <li><a id="Hurst2021">[Hurst2021]</a> <a name="Hurst2021"></a>Siekmann, Jonah, et al. "Sim-to-real learning of all common bipedal gaits via periodic reward composition." 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021</li>
        <li><a id="Duan2022">[Duan2022]</a> <a name="Duan2022"></a>Duan, Helei, et al. "Sim-to-Real Learning of Footstep-Constrained Bipedal Dynamic Walking." arXiv preprint arXiv:2203.07589 (2022)</li>
        <li><a id="Dao2022">[Dao2022]</a> <a name="Dao2022"></a>J. Dao, K. Green, H. Duan, A. Fern and J. Hurst, "Sim-to-Real Learning for Bipedal Locomotion Under Unsensed Dynamic Loads," 2022 International Conference on Robotics and Automation (ICRA), 2022, pp. 10449-10455</li>
        <li><a id="Raisim">[Raisim]</a> <a name="Raisim"></a>https://raisim.com/sections/Performance.html</li>
        <li><a id="Rudin2021">[Rudin2021]</a> <a name="Rudin2021"></a>Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In Proc. Conf. Robot Learn. (CoRL), pages 91–100, London, UK, November 2021</li>
        <li><a id="Margolis2022">[Margolis2022]</a> <a name="Margolis2022"></a>Margolis, Gabriel B., et al. "Rapid Locomotion via Reinforcement Learning." arXiv preprint arXiv:2205.02824 (2022)</li>
        <li></li>
	</ul>
	<div class="slide-footer"><br></div>
</section>
</section>



<style type="text/css"> div.flushright{text-align:right;} </style>
<script src="./reveal.js/plugin/chart/Chart.min.js"></script>
<script src="./reveal.js/plugin/chart/plugin.js"></script>
<script src="./reveal.js/plugin/mine/chartjs-plugin-colorschemes.js"></script>
<script src="./reveal.js/plugin/mine/bokeh-2.2.2.min.js"></script>
<script src="./reveal.js/plugin/mine/bokeh-widgets-2.2.2.min.js"></script>
<script src="./reveal.js/plugin/mine/bokeh-tables-2.2.2.min.js"></script>
<script type="text/javascript" sync src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js"></script>

<script src="./reveal.js/dist/reveal.js"></script>
<script src="./reveal.js/plugin/notes/notes.js"></script>
<script src="./reveal.js/plugin/search/search.js"></script>
<script src="./reveal.js/plugin/zoom/zoom.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration

Reveal.initialize({
controls: true,
progress: true,
history: true,
center: false,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: true,
fragmentInURL: true,
hashOneBasedIndex: false,
pdfSeparateFragments: false,

overview: true,

transition: 'fade',
transitionSpeed: 'default',
margin: 0.01,


// Optional libraries used to extend reveal.js
dependencies: [
{src:'./reveal.js/plugin/spotlight/spotlight.js'},
{src: './reveal.js/plugin/math/math-gc.js', async:true},
{src: './reveal.js/plugin/toc-progress/toc-progress.js', async: true, callback: function() { toc_progress.initialize('reduce', 'rgba(120,138,130,0.2)', 'body'); }}
],

chart: {
  defaults: {
	  	global:{
			defaultFontSize: 20,
			title:{
				display: true,
				fontColor: "black",
				fontSize: 40
			},
			scales: {
				scaleLabel: { fontColor: "black"}, 
				gridLines: { color: "#FFF", zeroLineColor: "#FFF" }
			} ,
			legend:{
					display: true,
					labels: {fontColor: "black", fontSize: 30 }
			}
		}
	}
},

plugins: [RevealNotes, RevealSearch, RevealZoom, RevealChart], spotlight: {toggleSpotlightOnMouseDown: false}, keyboard: {84: function() { RevealSpotlight.toggleSpotlight() }}, width: 1366, height: 768, margin: 0.1,
audioStartAtFragment: true,
  audio: {
    advance: -1, autoplay: false, defaultDuration: 0, defaultAudios: false, playerOpacity: 0.8, playerStyle: 'position: fixed; bottom: 9.5vh; left: 0%; width: 30%; height:30px; z-index: 33;' },
anything: [
        // Following initialization code for class animate from anything-demo.html.
        // Copyright (c) 2016 Asvin Goel, under The MIT License (MIT).
	{className: "animate",  initialize: (function(container, options){
		Reveal.addEventListener( 'fragmentshown', function( event ) {
			if (typeof event.fragment.beginElement === "function" ) {
				event.fragment.beginElement();
			}
		});
		Reveal.addEventListener( 'fragmenthidden', function( event ) {
			if (event.fragment.hasAttribute('data-reverse') ) {
				var reverse = event.fragment.parentElement.querySelector('[id=\"' + event.fragment.getAttribute('data-reverse') + '\"]');
				if ( reverse && typeof reverse.beginElement === "function" ) {
					reverse.beginElement();
				}
			}
		});
		if ( container.getAttribute("data-svg-src") ) {
			var xhr = new XMLHttpRequest();
			xhr.onload = function() {
				if (xhr.readyState === 4) {
					var svg = container.querySelector('svg');
					container.removeChild( svg );
					container.innerHTML = xhr.responseText + container.innerHTML;
					if ( svg ) {
						container.querySelector('svg').innerHTML = container.querySelector('svg').innerHTML + svg.innerHTML;
					}
				}
				else {
					console.warn( "Failed to get file. ReadyState: " + xhr.readyState + ", Status: " + xhr.status);
				}
			};
			xhr.open( 'GET', container.getAttribute("data-svg-src"), true );
			xhr.send();
		}
	}) },
	{className: "randomPic",
	 defaults: {imgalt: "Dummy alt text",
		    imgcaption: "Image by {name}",
		    choices: [ {name: "dummyname", path: "dummypath"} ]},
	 initialize: (function(container, options){
	     var choice = Math.trunc( Math.random()*(options.choices.length) );
	     var img = "<img src='" + options.choices[choice].path + "' alt='" + options.choices[choice].imgalt + "' />";
	     var caption = options.imgcaption.replace(new RegExp('\{name\}', 'gm'), options.choices[choice].name);
	     container.innerHTML = img + caption;
	 }) },
	{className: "notes",
	 initialize: (function(container, options){
	     container.addEventListener('click', function(e) { Reveal.getPlugins().notes.open(); });
	 }) }
],
coursemod: { enabled: true, shown: false },
});
</script>

<script>
Reveal.configure({
  keyboard: {
    39: 'next', // go to the next slide when the ENTER key is pressed
    37: 'prev' // don't do anything when SPACE is pressed (i.e. disable a reveal.js default binding)
  }
})
</script>
</body>
</html>
